\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{float}
\usepackage{framed}
\usepackage{mathrsfs}
\usepackage{cite}
\usepackage{url}
\usepackage{multirow}
\usepackage{siunitx}

\def\zt{\mathbb{Z}/t\mathbb{Z}}
\def\*#1{\mathbf{#1}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prb}{Problem}

\title{Algorithms for Evaluating Nonlinear Functions Under Homomorphic Encryption}
\author{Trevor Henderson}

\begin{document}

\maketitle

    \begin{abstract}

        This paper looks at methods of computing nonlinear functions efficiently
        under the constraints of homomorphic computation
        --- access to only addition and multiplication modulo $t$.
        I show that $\sqrt{t}$ multiplications are necessary 
        and $3\sqrt{t}$ multiplications are sufficient to evaluate
        an arbitrary single variable function.
        I then show that many multivariate functions, such as division
        and boolean comparisons can be evaluated using
        $O(\sqrt{t})$ multiplications.
        The multiplicative depth for both algorithms is $O(\log t)$.

        Experimental results utilizing an 
        implementation of the somewhat homomorphic scheme
        YASHE boast
        secure homomorphic evaluations of arbitrary
        8-bit single variable functions 
        in less than 4 milliseconds amortized
        on a 2012 MacBook Pro.
    \end{abstract}

    \section{Homomorphic Encryption Overview}
        Homomorphic encryption let's you encrypt messages
        and then
        do math on the encrypted messages.

        While there are many different homomorphic schemes,
        all of them rely on encrypting the plaintext with some sort of noise.
        As operations are performed on the ciphertexts this noise grows until,
        after a certain amount of computation,
        it becomes so large that the ciphertext can no longer be decrypted.
        A procedure called bootstrapping homomorphically decrypts a message,
        resetting its noise to some small constant.
        With bootstrapping, circuits of arbitrary depth can be evaluated.
        However bootstrapping has an immense computational overhead, 
        so in practice many schemes are somewhat homomorphic, 
        in that there computational depth is bounded~\cite{homenc}.

        Homomorphic encryption began as a very slow process where single 
        bit operations could take as much as half an hour to compute~\cite{firstImp}.
        Now however we can evaluate large circuits in seconds.
        But these circuits are very limited, 
        as they can only
        consist of addition and multiplication.
        Many operations used in a standard computing model
        like equality testing, bit shifting, floating point multiplication,
        and division cannot be computed naturally with just those two operations.
        Therefore this paper looks at efficient ways of computing
        such functions.

    \section{Problem Description}

    The goal is to evaluate functions $f:\zt\to\zt$ efficiently under homomorphic encryption.
    Setting aside the specifics of homomorphic schemes, 
    we describe the constraints of this problem as follows.
    \begin{enumerate}[1.]
        \item
            We only have access to addition and multiplication operations modulo the plaintext modulus $t$.
        \item
            Additions and scalar multiplications are relatively easy to compute where as nonscalar multiplications are much harder to compute. 
        \item
            Additions and scalar multiplications do not increase the noise of ciphertexts by much, whereas nonscalar multiplications greatly increase the noise of ciphertexts.
    \end{enumerate}

    Therefore in order to evaluate functions efficiently, the computation must minimize
    the total number of nonscalar multiplications --- to reduce the total computation time ---
    as well as the nonscalar multiplicative depth --- to reduce the computational overhead associated with noise growth.

    \subsection{Limitations of the Problem Description}
        Naturally, this description cannot cover all cases as there are many homomorphic schemes, some with additional properties that can ease computation.
        For example, some homomorphic schemes allow the individual bits of ciphertexts to be
        accessed and computed upon.
        This make
        it possible to implement standard computer hardware algorithms that rely on bit shifting and bit logic to perform operations like evaluating inequalities and performing division.

        Additionally, there are homomorphic schemes that support plaintext moduli so large that performing integer multiplications to maximum multiplicative depth will not cause overflow.
        Under these schemes one can perform floating point operations by keeping track of a radix index for each integer and scaling down the decrypted result appropriately~\cite{newton}.
        
        While these alternatives and others are freeing, they tend to come at a large cost.
        Oftentimes ciphertexts will only encrypt single integers, 
        losing orders of magnitude of computation time that could be gained with batch processing.
        The ciphertexts can become bloated to allow for room for overflow, increasing their size and the time it takes to perform computations on them immensely.

        Additionally, from a purely aesthetic standpoint, adding other helper variables 
        like a radix, 
        takes away from some of the magic of homomorphic computation. 
        It is far less amazing to unbox something half finished,
        than it is to have a solution appear all at once and ready to go.

        Still, these methods are useful in areas where complicated homomorphic functions must be calculated on only a few, large inputs but I will not discuss them further.

    \section{Solution Description}

        My solution to the problem works to form
        a more symbiotic relationship with the modulo operation that is built into the homomorphic addition and multiplication operations.
        I use it as a tool for reduction rather than a boundary to be avoided.

        Using this philosophy I derive a method to compute any single variable homomorphic function. 
        I prove this method is an asymptotic lower bound.
        Then I investigate several possible methods of computing multivariate functions.

        This method can very efficiently perform batch operations on relatively small 
        (8 or 16-bit) integers. 
        This makes it very useful for problems like image processing.

    \subsection{Notation}
        \begin{enumerate}[-]
            \item
                A function is \emph{computable} if it can be computed using only addition and multiplication operations modulo $t$.
            \item
                All homomorphically encrypted variables are represented in \textbf{bold}.
            \item
                All operations are assumed to modulo $t$.
        \end{enumerate}

    \section{Single Variable Functions}

    Let us define the function $\delta: \zt\to \{0,1\}$ as follows:

    \begin{align}
        \delta(x) 
        = \left\{
            \begin{array}{cc}
                1 & \text{if } x = 0\\
                0 & \text{if } x \neq 0
            \end{array}
        \right.
    \end{align}

    \begin{lem}
        If $\delta$ is computable, then any total function 
        $f:\zt\to\zt$ is computable.
    \end{lem}
    \begin{proof}
        Using the function $\delta$, any total function $f:\zt\to\zt$ can be computed as follows:
        \begin{align}
            f(x) = \sum_{k = 0}^{t-1} f(k)\delta(x - k)
        \end{align}
    \end{proof}

    \begin{lem}
        Exactly $t^{t}$ distinct total functions $f:\zt\to\zt$ exist. 
    \end{lem}
    \begin{proof}
        Each of the $|\zt|$ possible inputs to $f$ corresponds to 
        one of $|\zt|$ possible outputs, therefore there are
        \begin{align}
            |\zt|^{|\zt|} = t^t
        \end{align}
        total functions that exist. 
    \end{proof}

    \begin{lem}
        At most $t^{\varphi(t) + 1}$ distinct total functions $f:\zt\to\zt$ are computable.
    \end{lem}
    \begin{proof}
        By Euler's Theorem, $x^{\varphi(t) + 1} = x$, 
        therefore any polynomial $P:\zt\to\zt$ can be reduced to the form
        \begin{align}
          P(x) = \sum_{i = 0}^{\varphi(t)} a_{i}x^{i}
        \end{align}
        This reduced polynomial has $\varphi(t) + 1$ coefficients, 
        each of which take on one of $|\zt| = t$ values,
        therefore $t^{\varphi(t) + 1}$ such polynomials exist. 
        Any computation involving only addition and multiplication modulo $t$
        is equivalent to a polynomial modulo $t$, therefore at most
        $t^{\varphi(t) + 1}$ total functions can be computed. 
    \end{proof}
    
    \begin{lem}
        $\delta$ is computable if $t$ is prime.
    \end{lem}
    \begin{proof}
        If $t$ is prime then $\delta$ can be computed as follows:
        \begin{align}
            \delta(x) = 1 - x^{t - 1}
        \end{align}
        By Fermat's Little Theorem, if $x \neq 0$ then $x^{t - 1} = 1$ and so $\delta(x) = 0$.
        If $x = 0$, then $x^{t - 1} = 0$ and so $\delta(x) = 1$. 
    \end{proof}

    \begin{thm}
        All possible functions 
        $f:\zt\to\zt$ 
        can be computed if and only if $t$ is prime.
    \end{thm}
    \begin{proof}
        If $t$ is prime then $\delta$ can be computed by Lemma 4 so by Lemma 1, all functions can be computed.
        If $t$ is not prime then $\varphi(t) + 1 < t$. So by Lemmas 2 and 3
        there exist some functions which cannot be computed.
    \end{proof}

    \begin{thm}
        If $t$ is prime, then each function 
        $f:\zt\to\zt$ 
        is uniquely represented by a polynomial of degree less than $t$. 
    \end{thm}
    \begin{proof}
        There are $t^{\phi(t) +1} = t^t$ distinct polynomials and $t^t$ total functions, therefore every function must be uniquely represented by exactly one polynomial.
    \end{proof}

    From this point on we will consider the modulus $t$ to be prime.
    We can calculate the minimal polynomial representing a function as follows:
    \begin{align}
        P(x) = \sum_{k = 0}^{t-1} f(k)\delta(x - k)\pmod{x^t - x}
    \end{align}

    This polynomial can be calculated in the clear so it has negligible impact on running time.
    As this polynomial is 
    the only polynomial with degree less than $t$ equivalent to $f$,
    there is no faster way to calculate $f(x)$ than by evaluating $P$ at $x$.
    Paterson and Stockmeyer~\cite{paterson} proved that evaluating an arbitrary polynomial of degree
    $t$ requires at least $\sqrt{t}$ nonscalar multiplications.

    Algorithm~\ref{evalSinglePoly} is a modification of Paterson and Stockmeyer's Algorithm B which maintains a logarithmic multiplicative depth.
    It requires approximately $3\sqrt{t}$ nonscalar multiplications and has a multiplicative depth of
    approximately $\log_2 t$.

    \begin{algorithm}
        \caption{Evaluate a single variable polynomial}\label{evalSinglePoly}
        \begin{algorithmic}
            \Function{EvaluatePolynomial}{$\mathbf{x}$, $P$}
                \\
                \State{
                  Let $P$ be a polynomial $a_0 + a_1x + \cdots + 
                  a_{t}x^{t}$ with 
                  $t = m^{2} - 1$
                }
                \\
                \State{
                  $\mathbf{X}[0] \leftarrow 1$, $\mathbf{X}[1] \leftarrow \mathbf{x}$
                }
                \Comment{$\mathbf{X}[i] = \mathbf{x}^i$}
                \\
                \For{$i = 2$ to $m$}
                    \State{
                      $\mathbf{X}[i] = 
                      \mathbf{X}\left[\left\lfloor\frac{i}{2}\right\rfloor\right]
                      \cdot
                      \mathbf{X}\left[\left\lceil\frac{i}{2}\right\rceil\right]$
                    }
                    \Comment{
                      Compute powers $\mathbf{x}^2 \dots \mathbf{x}^{m}$
                    }
                \EndFor%
                \\
                \For{$i = 2$ to $m-1$}
                    \State{
                      $\mathbf{X}[im] = 
                      \mathbf{X}\left[\left\lfloor\frac{i}{2}\right\rfloor m\right]
                      \cdot
                      \mathbf{X}\left[\left\lceil\frac{i}{2}\right\rceil m\right]$
                    }
                    \Comment{
                      Compute powers $\mathbf{x}^{2m}\dots \mathbf{x}^{(m-1)m}$
                    }
                \EndFor%
                \\
                \State{$\mathbf{p} \leftarrow 0$}
                \Comment{Compute the polynomial as}
                \For{$i = 0$ to $m-1$}
                \Comment{
                  $\sum_{i = 0}^{m-1}\mathbf{x}^{im}\left(\sum_{j = 0}^{m - 1}a_{im + j}\mathbf{x}^j\right)$
                }
                \State{$\mathbf{q} \leftarrow 0$}
                %\Comment
                %$(a_0 + a_1x + \dots a_{m-1}x^{m-1})$
                    \For{$j = 0$ to $m - 1$}
                %\Comment $+ (a_m + a_{m + 1}x + \dots a_{2m - 1}x^{m-1})x^m$
                    \State{$\mathbf{q} \leftarrow \mathbf{q} + a_{im + j}\cdot \mathbf{X}[j]$}
                    \EndFor%
                    \State{$\mathbf{p} \leftarrow \mathbf{p} + \mathbf{q} \cdot \mathbf{X}[im]$}
                \EndFor%
                \State\Return{$\mathbf{p}$}
                \\
            \EndFunction%
        \end{algorithmic}
    \end{algorithm}

    Therefore any single variable function can be computed homomorphically with no more than
    $3\sqrt{t}$ nonscalar multiplications and a multiplicative depth of no more than $\log_2 t$.
    This is asymptotically equal to the minimum amount of nonscalar multiplications needed to compute an arbitrary single variable function.

    \section{Multivariate Functions}
    \subsection{Brute Force}
        Mimicking the single variable case,
        we can compute any bivariate function by evaluating the polynomial
        \begin{align}
            P(x,y)
            =
            \sum_{k = 0}^{t - 1}
            \sum_{l = 0}^{t - 1}
            f(k, l)
            \mathbf{\delta}(x - k)
            \mathbf{\delta}(y - l)
            \pmod{x^t - x}
            \pmod{y^t - y}
        \end{align}
        This polynomial has $O(t^2)$ coefficients. 
        A trivial algorithm for computing this polynomial is shown in Algorithm~\ref{multivariatePoly}.
        \begin{algorithm}
            \caption{Evaluate a multivariate polynomial}\label{multivariatePoly}
            \begin{algorithmic}
                \Function{EvaluatePolynomial}{$\mathbf{x}$, $\mathbf{y}$, $P$}
                    %\State On input $\mathbf{x}$ and $[a_0\dots a_n]$ with $n = m^2 - 1
                    \\
                    \State{
                      Let $P$ be a polynomial 
                      $a_{0,0} +  
                      \cdots + a_{i,j}x^{i}y^j + 
                      \cdots + 
                      a_{t,t}x^{t}y^t$
                    }
                    \\
                    \State{
                      $\mathbf{X}[0] \leftarrow 1$, $\mathbf{X}[1] \leftarrow \mathbf{x}$
                    }
                    \Comment{$\mathbf{X}[i] = \mathbf{x}^i$}
                    \State{
                      $\mathbf{Y}[0] \leftarrow 1$, $\mathbf{Y}[1] \leftarrow \mathbf{x}$
                    }
                    \Comment{$\mathbf{Y}[i] = \mathbf{x}^i$}
                    \\
                    \For{$i = 2$ to $t$}
                        \State{
                          $\mathbf{X}[i] = 
                          \mathbf{X}\left[\left\lfloor\frac{i}{2}\right\rfloor\right]
                          \cdot
                          \mathbf{X}\left[\left\lceil\frac{i}{2}\right\rceil\right]$
                        }
                        \Comment{Compute powers $\mathbf{x}^2 \dots \mathbf{x}^{t}$}
                        \State{
                          $\mathbf{Y}[i] = 
                          \mathbf{Y}\left[\left\lfloor\frac{i}{2}\right\rfloor\right]
                          \cdot
                          \mathbf{Y}\left[\left\lceil\frac{i}{2}\right\rceil\right]$
                        }
                        \Comment{Compute powers $\mathbf{y}^2 \dots \mathbf{y}^{t}$}
                    \EndFor%



        %$x^2\dots x^{t - 1}$ and $y^2\dots y^{t - 1}$. T

                    %\State let $a_{ij}$ be the coefficient of term $x^iy^j$
                    \\
                    \State{$p \leftarrow 0$}
                    \For{$i = 0$ to $t - 1$}
                        \For{$j = 0$ to $t - 1$}
                          \State{$p \leftarrow p + a_{ij}\mathbf{X}[i]\mathbf{Y}[j]$}
                          \Comment{Multiply coefficients and variables}
                        \EndFor%
                    \EndFor%
                    \State\Return{$\mathbf{p}$}
                    \\
                \EndFunction%
            \end{algorithmic}
        \end{algorithm}

        This algorithm requires $O(t^2)$ multiplications and has a multiplicative depth of
        $\log_2 t + 1$.
        The multivariate case is indeed much slower than the univariate case as the intertwined variables make it difficult
        to decrease the total number of nonscalar multiplications without greatly 
        increasing the nonscalar multiplicative depth.
        Additionally, for functions with even more variables, this method quickly suffers the curse of dimensionality.
        For extremely random two dimensional functions
        brute force might be the only option,
        however as we will see most practical functions can be evaluated much faster.

    \subsection{Linearly separable functions}
        Many functions, although not linear, can be evaluated as a combination of single variable functions and linear operations.
        For example the following common functions are linearly separable:
        \begin{enumerate}[-]
            \item
                \textsc{Equal To}:
                \begin{align}
                    \left[\mathbf{x} = \mathbf{y}\right]
                    =
                    \left[\mathbf{x} - \mathbf{y} = 0\right]
                \end{align}
            \item
                \textsc{Greater Than}:
                \\
                Let
                \begin{align}
                    \mathbf{a} 
                    &= \left[\mathbf{x}\geq \frac{t-1}{2}\right]
                    \\
                    \mathbf{b} 
                    &= \left[\mathbf{y} \leq \frac{t-1}{2}\right]
                    \\
                    \mathbf{c} &= \left[(\mathbf{x} - \mathbf{y} \mod{t}) \leq \frac{t-1}{2}\right]
                \end{align}
                %Where
                %\begin{align}
                    %[\omega]
                    %= \left\{
                        %\begin{array}{cc}
                            %1 & \text{if } \omega \text{ is true}\\
                            %0 & \text{if } \omega \text{ is false}\\
                        %\end{array}
                    %\right.
                %\end{align}
                Then
                \begin{align}
                    [\mathbf{x} \geq \mathbf{y}] 
                    %&= \mathbf{a}\mathbf{b} + \mathbf{c}(\mathbf{a}(1-\mathbf{b}) + (1-\mathbf{a})\mathbf{b})
                    %\\
                    &=
                    \mathbf{a}\mathbf{b} + \mathbf{c}(\mathbf{a} + \mathbf{b} - 2\mathbf{a}\mathbf{b})
                \end{align}
            \item
                \textsc{Division}:
                \begin{align}
                    \frac{\mathbf{x}}{\mathbf{y}} = \exp(\log\mathbf{x} - \log\mathbf{y})
                \end{align}
            \item
                \textsc{Logarithm}:
                \begin{align}
                    \log_{\mathbf{b}}\mathbf{x} = \exp(\log\log\mathbf{x} - \log\log\mathbf{b})
                \end{align}
            \item
                \textsc{Exponentiation}:
                \begin{align}
                    \mathbf{x}^\mathbf{y} = \exp(\mathbf{y}\log\mathbf{x})
                \end{align}
        \end{enumerate}

        The boolean expressions evaluate to either integers, either 0 or 1.
        However, division, logarithm and exponentiation algorithms 
        evaluate to real numbers, so they must be modified before they can be
        used homomorphically. 
        Consider the case of division. 
        Since we can only perform integer operations, we lose resolution when taking logarithms.
        To counteract this we can inflate the output of the logarithmic functions
        to take advantage of the entire range.
        We also must account for the fact that the subtraction is done modulo $t$, 
        so we will need to leave an extra bit to check if $y > x$.
        Finally we must choose the what should happen in the case of division by zero.
        Algorithm~\ref{divis} demonstrates these modifications and chooses to return the numerator if the denominator is zero.

    \begin{algorithm}
        \caption{Division}\label{divis}
        \begin{algorithmic}
            \Function{log}{$\mathbf{x}$}
                \If{$\mathbf{x} = 0$}
                    \State\Return{0}
                \Else%
                    \State\Return{
                        $
                        \left\lfloor
                        \frac{t}{2}\log_t\mathbf{x}
                        \right\rceil
                        $
                    }
                \EndIf%
            \EndFunction%
            \\
            \Function{exp}{$\mathbf{x}$}
                \If{$\mathbf{x} > \frac{t}{2}$}
                    \State\Return{0}
                \Else%
                    \State\Return{$
                        \left\lfloor
                        t^{\frac{2\mathbf{x}}{t}}
                        \right\rfloor
                    $}
                \EndIf%
            \EndFunction%
            \\
            \Function{div}{$\mathbf{x}$, $\mathbf{y}$}
                \State\Return{$
                    \textsc{exp}(
                    \textsc{log}(\mathbf{x}) 
                    - 
                    \textsc{log}(\mathbf{y}) 
                    )
                $}
            \EndFunction%
        \end{algorithmic}
    \end{algorithm}
        An almost identical algorithm can also be used to compute $\log_\mathbf{b}\mathbf{x}$.
        A homomorphic algorithm to compute exponentiation depends largely on the desired response when $\mathbf{x}^\mathbf{y} \geq t$.
        One thing to note is that division, logarithms and exponentiation computed this way will be approximate. Improving these approximations is discussed in the next section.
        
    \section{Future work}
        \subsection{Optimization of linearly separable functions}
          In certain applications an approximate algorithms
          will not be acceptable.
          An interesting problem would be to see if these approximations
          could be improved upon automatically using machine learning.
          For example, the optimization problem for division could be formulated as follows:

          \begin{prb}
            Given the form $F(x,y) = f_1\left( f_2(x) + f_3(y) \right)$, determine $f_1$, $f_2$, $f_3$ that minimize 
            \begin{align}
              \sum_{0\leq x,y<t} \left(F(x,y) 
              - \left\lfloor\frac{x}{y}\right\rfloor\right)^2
            \end{align}
          \end{prb}

          Here $f_1$, $f_2$, and $f_3$ could be considered arrays of size $t$ with integers in $\zt$, 
          therefore the search space is finite albeit very large.
          Simple hill climbing on these functions from the assignments produced by 
          Algorithm~\ref{divis} can make mild improvements.
          However my attempts to use other methods including simulated annealing and genetic algorithms have not yet been successful.
          %I suspect though have not proved that the problem is NP hard.
        %maximum common subgraph isomorphism problem
            
        \subsection{Non linearly separable functions}
            In order to evaluate functions that are not naturally separable, 
            machine learning techniques 
            could be used to find approximate them.
            Neural networks for example could be implemented 
            to approximate nonlinear functions.
            Neural networks rely on three operations: 
            addition, multiplication and the sigmoid function.
            The sigmoid function is a function of a single variable 
            therefore it could be implemented homomorphically 
            using the methods described above.
    \section{Experimental Results}

        The algorithms described in this paper have been implemented in C++ 
        under the
        somewhat homomorphic encryption scheme YASHE (Yet Another Somewhat Homomorphic Encryption scheme)~\cite{yashe}.
        The library is built upon Victor Shoup's number theoretic library C++ NTL~\cite{ntl}.

        The system is determined by the plaintext modulus $t$, the batch size $\beta$ and the security parameter $\lambda$.
        Hidden parameters for the YASHE scheme that determine the batch size and security parameter and the cylotomic degree $d$, and the ciphertext modulus $q$.
        Batches are made using the Chinese Remainder Theorem and batch size is equal to the number of factors modulo $t$ of the $d$th cyclotomic polynomial.
        For the system to be secure both $d$ and $q$ must be large.
        The degree of the $d$th cyclotomic polynomial is $n = \varphi(d)$, where $\varphi$ is Euler's Toitient function. We set the standard deviation of the ciphertext noise $\sigma_{err} = 8$ to be constant.

        I have investigated YASHE parameters for both 8 and 16 bit integers.
        %and 16 bit integers.
        I use the Fermat primes 
        $t = 2^8 + 1$ 
        and $t = 2^{16} + 1$ as moduli for 8 and 16 bit integers respectively.
        Through manual search I have found values of $d$ 
        with maximal batch sizes that
        large enough to be secure and provide a large multiplicative depth,
        while not being so large as to be computationally infeasible.
        %the largest batch sizes given
        %that they are are large enough to be secure
        %and not so large as to be computationally infeasible.

        \begin{center}
            \begin{tabular}{| r | r | r |}
                \hline
                $t$ & $d$ & Batch
                \\ \hline
                $2^8 + 1$ & 22016 & 5376
                \\ \hline
                $2^8 + 1$ & 66048 & 10752
                \\ \hline
                $2^{16} + 1$ & 32768 & 16384
                \\ \hline
                $2^{16} + 1$ & 65536 & 32768
                \\ \hline
            \end{tabular}
        \end{center}


        For those batch sizes, 
        we calculate the maximum value of $\log_2{q}$ that guarantees
        $\lambda$ bits of security with the following equation from~\cite{comparison}:
        \begin{align}
            \log_2(q) 
            \leq \min_{m> n}
            \frac{
                m^2\cdot\log_2(\gamma(m)) 
                + m\cdot
                \log_2\left(
                    \sigma_{err}
                    /
                    \sqrt{\lambda\log(2)/\pi}
                \right)
            }{
                m - n
            }
        \end{align}
        I found the values of the minimal root Hermite factor $\gamma(m)$ by interpolating the values of
        $\gamma(m)$ found in Table 1 of~\cite{comparison}
        The following is a table of the derived values:

        \begin{center}
            \begin{tabular}{| r | r || r | r | r | r |}
                \cline{3-6}
                \multicolumn{2}{c||}{max}
                & \multicolumn{4}{|c|}{Batch}
                \\ 
                \cline{3-6}
                \cline{3-6}
                \multicolumn{2}{c||}{$\log_2q$}
                     & 5376  & 10752 & 16384 & 32768 
                \\ \hline\hline
                \multirow{3}{*}{$\lambda$} & 64  & 584.11 & 1195.53 & 992.54 & 1850.70
                \\ \cline{2-6}
                & 80   & 437.49 & 1055.43 & 878.60 & 1627.87
                \\ \cline{2-6}
                & 128  & 389.82 &  791.51 & 658.27 & 1218.44
                \\ \hline
            \end{tabular}
        \end{center}

        I have chosen to perform timing tests on the following
        sets of parameters which are each optimized to have a
        multiplicative depth just large enough to support a particular operation.
        I have found that setting the radix $w$ such that
        $\log_2 w \approx \frac{\log_2 q}{6}$ to be a good trade off
        between running time and multiplicative depth.

        \begin{center}
            \begin{tabular}{| r | r | r | r | r | r | c |}%
                \hline
                $\#$& $t$ & $\log_2 q$ & $d$ & $\log_2w$ & $\lambda$ & Operation
                \\ \hline
                1 & $2^8 + 1$ & 438 & 22016 & 74 & $\approx 80$ & Polynomial
                \\ \hline
                2 & $2^8 + 1$ & 546 & 22016 & 92 & $>64$ & Inequality
                \\ \hline
                3 & $2^8 + 1$ & 930 & 66048 & 156 & $>80$ & Division
                \\ \hline
            \end{tabular}
        \end{center}

        The following timing results were computed on a 2012 Macbook Pro.
        NTL was not built with threading.
        All of the tests have been averaged over 10 trials.

        %\begin{center}
            %\begin{tabular}{| r | r | r | r |}
                %\hline
                %\multicolumn{4}{|c|}{Max Multiplicative Depth}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Depth
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 15
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 11
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 9
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 & 33
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 & 28
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 & 21
                %\\ \hline
            %\end{tabular}
        %\end{center}

        %\begin{center}
            %\begin{tabular}{| r | r | r | r | r |}
                %\hline
                %\multicolumn{5}{|c|}{Encryption}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Time/Batch & Time/Operation
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 6917 ms & 1.287 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 6858 ms & 1.276 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 6850 ms & 1.274 ms
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 & 39146 ms & 3.641 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 & 38442 ms & 3.575 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 & 37983 ms & 3.533 ms
                %\\ \hline
            %\end{tabular}
        %\end{center}

        %\begin{center}
            %\begin{tabular}{ | r | r | r | r | r |}
                %\hline
                %\multicolumn{5}{|c|}{Decryption}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Time/Batch & Time/Operation
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 10400 ms & 1.934 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 10359 ms & 1.927 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 10335 ms & 1.923 ms
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 & 43997 ms & 4.092 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 & 43512 ms & 4.047 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 & 42648 ms & 3.967 ms
                %\\ \hline
            %\end{tabular}
        %\end{center}

        %\begin{center}
            %\begin{tabular}{ | r | r | r | r | r |}
                %\hline
                %\multicolumn{5}{|c|}{Addition}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Time/Batch & Time/Operation
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 0.580 ms & 0.00011 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 0.455 ms & 0.00008 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 0.411 ms & 0.00008 ms
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 & 7.003 ms & 0.00065 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 & 6.358 ms & 0.00059 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 & 5.246 ms & 0.00049 ms
                %\\ \hline
            %\end{tabular}
        %\end{center}

        %\begin{center}
            %\begin{tabular}{ | r | r | r | r | r |}
                %\hline
                %\multicolumn{5}{|c|}{Multiplication}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Time/Batch & Time/Operation
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 1331 ms & 0.248 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 769 ms & 0.143 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 644 ms & 0.120 ms
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 &  ms & 1.096 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 &  ms & 0.828 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 &  ms & 0.494 ms
                %\\ \hline
            %\end{tabular}
        %\end{center}
        
        %\begin{center}
            %\begin{tabular}{ | r | r | r | r | r |}
                %\hline
                %\multicolumn{5}{|c|}{Polynomial of degree $t - 1$}
                %\\ \hline
                %$t$ & $\lambda$ & Batch & Time/Batch & Time/Operation
                %\\ \hline
                %$2^8 + 1$ & 64 & 5376 & 1331 ms & 11.025 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 5376 & 769 ms & 6.364 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 5376 & 644 ms & 5.346 ms
                %\\ \hline
                %$2^8 + 1$ & 64 & 10752 &  ms & 44.435 ms
                %\\ \hline
                %$2^8 + 1$ & 80 & 10752 &  ms & 33.841 ms
                %\\ \hline
                %$2^8 + 1$ & 128 & 10752 &  ms & 19.954 ms
                %\\ \hline
            %\end{tabular}
        %\end{center}

        \begin{center}
            \begin{tabular}{| c | r | r |}
                \hline
                $\#$ & Time/Batch & Time/Operation
                \\ \hline
                \multicolumn{3}{|c|}{Encryption}
                \\ \hline
                $1$ & \SI{6659.2}{\milli\second} & \SI{1.239}{\milli\second}
                \\ \hline
                $2$ & \SI{6648.9}{\milli\second} & \SI{1.237}{\milli\second}
                \\ \hline
                $3$ & \SI{38263.2}{\milli\second} & \SI{3.559}{\milli\second}
                \\ \hline
                \multicolumn{3}{|c|}{Decryption}
                \\ \hline
                $1$ & \SI{9938.4}{\milli\second} & \SI{1.849}{\milli\second}
                \\ \hline
                $2$ & \SI{9958.3}{\milli\second} & \SI{1.852}{\milli\second}
                \\ \hline
                $3$ & \SI{39974.1}{\milli\second} & \SI{3.718}{\milli\second}
                \\ \hline
                \multicolumn{3}{|c|}{Addition with Constant}
                \\ \hline
                $1$ & \SI{18.4}{\micro\second} & \SI{3.423}{\nano\second}
                \\ \hline
                $2$ & \SI{16.6}{\micro\second} & \SI{3.088}{\nano\second}
                \\ \hline
                $3$ & \SI{27.2}{\micro\second} & \SI{2.530}{\nano\second}
                \\ \hline
                \multicolumn{3}{|c|}{Multiplication with Constant}
                \\ \hline
                $1$ & \SI{1974.2}{\micro\second} & \SI{367.2}{\nano\second}
                \\ \hline
                $2$ & \SI{2186.9}{\micro\second} & \SI{406.8}{\nano\second}
                \\ \hline
                $3$ & \SI{5496.7}{\micro\second} & \SI{511.1}{\nano\second}
                \\ \hline
                \multicolumn{3}{|c|}{Addition of Ciphertexts}
                \\ \hline
                $1$ & \SI{417.6}{\micro\second} & \SI{77.679}{\nano\second}
                \\ \hline
                $2$ & \SI{501.6}{\micro\second} & \SI{93.304}{\nano\second}
                \\ \hline
                $3$ & \SI{6099.4}{\micro\second} & \SI{567.281}{\nano\second}
                \\ \hline
                \multicolumn{3}{|c|}{Multiplication of Ciphertexts}
                \\ \hline.
                $1$ & \SI{462.0}{\milli\second} & \SI{85.932}{\micro\second}
                \\ \hline
                $2$ & \SI{560.7}{\milli\second} & \SI{104.288}{\micro\second}
                \\ \hline
                $3$ & \SI{2306.4}{\milli\second} & \SI{214.511}{\micro\second}
                \\ \hline
                \multicolumn{3}{|c|}{Random Single Variable Function}
                \\ \hline
                $1$ & \SI{20248.2}{\milli\second} & \SI{3.766}{\milli\second}
                \\ \hline
                $2$ & \SI{25029.7}{\milli\second} & \SI{4.656}{\milli\second}
                \\ \hline
                $3$ & \SI{91361.1}{\milli\second} & \SI{8.497}{\milli\second}
                \\ \hline
                \multicolumn{3}{|c|}{Comparison of Ciphertexts}
                \\ \hline
                $2$ & \SI{71071.4}{\milli\second} & \SI{13.220}{\milli\second}
                \\ \hline
                $3$ & \SI{300576}{\milli\second} & \SI{27.955}{\milli\second}
                \\ \hline
                \multicolumn{3}{|c|}{Division of Ciphertexts}
                \\ \hline
                $3$ & \SI{294826}{\milli\second} & \SI{27.688}{\milli\second}
                \\ \hline
            \end{tabular}
        \end{center}

    %\begin{algorithm}
        %\caption{Logarithm}
        %\begin{algorithmic}
            %\Function{loglog}{$\mathbf{x}$}
                %\If {$\mathbf{x} = 0$}
                    %\State\Return{0}
                %\Else
                    %\State\Return{
                        %$
                        %\left\lfloor
                        %\frac{t}{2}\left(
                            %\frac{\log\log\mathbf{x}}{\log\log t}
                        %\right)
                        %\right\rceil
                        %$
                    %}
                %\EndIf
            %\EndFunction
            %\\
            %\Function{exp}{$\mathbf{x}$}
                %\If {$\mathbf{x} > \frac{t}{2}$}
                    %\State\Return{0}
                %\Else
                    %\State\Return {$
                        %\left\lfloor
                            %t^{\frac{2\mathbf{x}}{t}}
                        %\right\rceil
                    %$}
                %\EndIf
            %\EndFunction
            %\\
            %\Function{log}{$\mathbf{b}$, $\mathbf{x}$}
                %\State\Return{$
                    %\textsc{exp}(
                    %\textsc{loglog}(\mathbf{x}) 
                    %- 
                    %\textsc{loglog}(\mathbf{b}) 
                    %)
                %$}
            %\EndFunction
        %\end{algorithmic}
    %\end{algorithm}

    \nocite{*}
    \bibliographystyle{plain}
    \bibliography{biblio}{}

\end{document}
